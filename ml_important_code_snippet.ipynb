{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi9wbXjxv7Y54XpZUXXn1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep1847e/Phy654-data/blob/main/ml_important_code_snippet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G-nKHSqgprA"
      },
      "outputs": [],
      "source": [
        "# to read and store data from root file\n",
        "def get_features_labels(file_name):\n",
        "    # load file\n",
        "    h5file = tables.open_file(file_name, 'r')\n",
        "    njets = getattr(h5file.root,features[0]).shape[0]\n",
        "\n",
        "    # allocate arrays\n",
        "    feature_array = np.zeros((njets,nfeatures))\n",
        "    label_array = np.zeros((njets,nlabels))\n",
        "\n",
        "    # load feature arrays\n",
        "    for (i, feat) in enumerate(features):\n",
        "        feature_array[:,i] = getattr(h5file.root,feat)[:]\n",
        "\n",
        "    # load labels arrays\n",
        "    for (i, label) in enumerate(labels):\n",
        "        prods = label.split('*')\n",
        "        prod0 = prods[0]\n",
        "        prod1 = prods[1]\n",
        "        fact0 = getattr(h5file.root,prod0)[:]\n",
        "        fact1 = getattr(h5file.root,prod1)[:]\n",
        "        label_array[:,i] = np.multiply(fact0,fact1)\n",
        "\n",
        "    feature_array = feature_array[np.sum(label_array,axis=1)==1]\n",
        "    label_array = label_array[np.sum(label_array,axis=1)==1]\n",
        "\n",
        "    h5file.close()\n",
        "    print (feature_array)\n",
        "    print ('\\n\\n')\n",
        "    print (label_array)\n",
        "    return feature_array, label_array\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FROM ASSIGNMENT**"
      ],
      "metadata": {
        "id": "JqTTQjhaqViI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all imports\n",
        "# 1 for creating a multi class classification model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler ,LabelEncoder\n",
        "from sklearn.metrics import accuracy_score ,confusion_matrix,classification_report,recall_score\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 2\n"
      ],
      "metadata": {
        "id": "_bwHziiZhJ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download and store data in Dataframe just give the url\n",
        "import gdown\n",
        "url = \"https://swmukher.web.cern.ch/swmukher/dataset_star.csv\"\n",
        "output = 'star_data.csv' #data will be saved as star_data.csv\n",
        "gdown.download(url, output, quiet=True)\n",
        "df=pd.read_csv(\"star_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JXL2rn9DkcVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when there are multiple classes we can use this to store\n",
        "# a particular class data in one dataframe\n",
        "type_0 = df[df['Type']==0]"
      ],
      "metadata": {
        "id": "sBj19dnak4ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for plotting histogram\n",
        "plt.hist((type_0['Temperature']),color='blue',rwidth=1.5,alpha=0.5,\n",
        "         bins=10,edgecolor='black') # CAN USE density='True'\n",
        "# here alpha is for transparency and rwidth is for width of the bars\n",
        "plt.yscale('log') #for scalking"
      ],
      "metadata": {
        "id": "uabOq51MlQV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Spectral_Class'].unique() #to see unique elements of a feature"
      ],
      "metadata": {
        "id": "elFI4CRAmHFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Spectral_Class'] = LabelEncoder().fit_transform(df['Spectral_Class'])\n",
        "# to change that non numerical feature to numerical\n",
        "df['Color'] = df['Color'].str.lower()\n",
        "# to change from upper to lower value\n",
        "# to standarize the input features\n",
        "Ifeatures=['Temperature','L','R','A_M','Color','Spectral_Class']\n",
        "df[Ifeatures]=StandardScaler().fit_transform(df[Ifeatures])\n",
        "# one-hots encoding\n",
        "Y=to_categorical(Y,num_classes=6)"
      ],
      "metadata": {
        "id": "11fYATlCmP1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data using sklearn\n",
        "Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.2,random_state=32)"
      ],
      "metadata": {
        "id": "9h_Cl58HnqMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fully connected layer\n",
        "model=Sequential()\n",
        "model.add(Dense(32,input_dim=Xtrain.shape[1],activation='relu')) #input layer\n",
        "model.add(Dense(16,activation='relu')) #hidden layer\n",
        "model.add(Dense(6,activation='softmax')) #output layer\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy','recall'])\n",
        "history=model.fit(Xtrain,Ytrain,epochs=100,batch_size=64,validation_split=0.20,verbose=2)"
      ],
      "metadata": {
        "id": "lwpM4_ZfnyBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # predicting on test data\n",
        "Ypred = model.predict(Xtest)\n",
        "Ypred_classes=np.argmax(Ypred,axis=1)\n",
        "Ytrue_classes=np.argmax(Ytest,axis=1)\n",
        "correct=Ypred_classes == Ytrue_classes\n",
        "\n",
        "ConfusionMatrix=confusion_matrix(Ytrue_classes,Ypred_classes)\n",
        "import seaborn as sns\n",
        "sns.heatmap(ConfusionMatrix,annot=True,fmt='d',cmap='Blues')\n",
        "\n",
        "accuracy=accuracy_score(Ytrue_classes,Ypred_classes) #for accuracy score\n",
        "plt.plot(history.history['loss'], label='Train Loss')"
      ],
      "metadata": {
        "id": "5WqlBEDzoaZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FROM HBB**"
      ],
      "metadata": {
        "id": "xFs2V8s_qRGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import tables\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "D3pCX1u3sg3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to select only rows wihch are signal and only background in a new array respectively\n",
        "f_sig = feature_array[y==1]\n",
        "f_bkg = feature_array[y==0]"
      ],
      "metadata": {
        "id": "6LeZGa5aqBpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FULLY CONNECTED MODEL\n",
        "inputs = Input(shape=(nfeatures,), name = 'input')\n",
        "x = Dense(64, name = 'dense_1', activation='relu')(inputs)\n",
        "x = Dense(32, name = 'dense_2', activation='relu')(x)\n",
        "x = Dense(32, name = 'dense_3', activation='relu')(x)\n",
        "outputs = Dense(nlabels, name = 'output', activation='softmax')(x)\n",
        "keras_model = Model(inputs=inputs, outputs=outputs)\n",
        "keras_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(keras_model.summary())"
      ],
      "metadata": {
        "id": "yL7yNJ_Gq7e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "model_checkpoint = ModelCheckpoint('keras_model_best.keras', monitor='val_loss', save_best_only=True)\n",
        "callbacks = [early_stopping, model_checkpoint]\n",
        "\n",
        "# fit keras model\n",
        "keras_model.fit(feature_array, label_array, batch_size=1024,\n",
        "                epochs=100, validation_split=0.2, shuffle=False,\n",
        "                callbacks = callbacks)"
      ],
      "metadata": {
        "id": "0_3iK3bUr6jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload best weights\n",
        "keras_model.load_weights('keras_model_best.keras')\n",
        "\n",
        "# run model inference on test data set\n",
        "predict_array_test = keras_model.predict(feature_array_test)\n",
        "\n",
        "# create ROC curve\n",
        "fpr, tpr, threshold = roc_curve(label_array_test[:,1], predict_array_test[:,1])\n",
        "\n",
        "# plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, lw=2.5, label=\"AUC = {:.1f}%\".format(auc(fpr,tpr)*100))\n",
        "plt.ylabel(r'True positive rate')\n",
        "plt.xlabel(r'False positive rate')\n",
        "#plt.semilogy()\n",
        "plt.ylim(0.001,1)\n",
        "plt.xlim(0,1)\n",
        "plt.grid(True)\n",
        "\n"
      ],
      "metadata": {
        "id": "qFEBjOzGvbjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FITTING PARAMETERS**"
      ],
      "metadata": {
        "id": "ndcl-E9hwNJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n"
      ],
      "metadata": {
        "id": "LWB7bebUwQ0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define ftrue(x)\n",
        "# define ftheta(theta,x)\n",
        "# define fthetagrad(theta,x)\n",
        "# create sample points of x\n",
        "# Get randomly sampled x values\n",
        "def samples(nsamples,width):\n",
        "    return(width*np.random.randn(nsamples))\n",
        "\n",
        "# cost function\n",
        "def get_avg_cost(theta0s,theta1s,nsamples, width):\n",
        "    n0=len(theta0s)\n",
        "    n1=len(theta1s)\n",
        "    C=np.zeros([n0,n1])\n",
        "    for j0 in range(n0):\n",
        "        for j1 in range(n1):\n",
        "            theta=np.array([theta0s[j0],theta1s[j1]])\n",
        "            x=samples(nsamples,width)\n",
        "            C[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
        "    return(C)\n",
        "\n",
        "\n",
        "# Plot the cost function landscape\n",
        "theta0s=np.linspace(-3,6,40)\n",
        "theta1s=np.linspace(-2,3,40)\n",
        "print (len(theta0s))\n",
        "print (len(theta1s))\n",
        "C=get_avg_cost(theta0s,theta1s,10000, 2.)\n",
        "nlevels=20\n",
        "X,Y=np.meshgrid(theta0s,theta1s,indexing='ij')\n",
        "plt.contourf(X,Y,C,nlevels)\n",
        "#plt.contour(X,Y,C,nlevels,colors=\"white\")\n",
        "plt.xlabel(\"theta_0\")\n",
        "plt.ylabel(\"theta_1\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "#print (C)\n",
        "\n",
        "# Gradient descent\n",
        "# take arbitrary parameters as starting point in a given range\n",
        "theta0_range = 1.\n",
        "theta1_range = 2.\n",
        "theta=np.array([theta0_range,theta1_range])*np.random.rand(2)\n",
        "print (theta)\n",
        "alpha=0.2 # \"learning rate\" (gradient descent step size)\n",
        "nsamples=50\n",
        "nsteps=90\n",
        "\n",
        "x_sweep=np.linspace(-4,4,300)\n",
        "xrange = 2.\n",
        "\n",
        "for n in range(nsteps):\n",
        "\n",
        "    x=samples(nsamples, xrange) # get random samples\n",
        "\n",
        "    # deviation from true function (vector):\n",
        "    deviation=f(theta,x)-true_f(x)\n",
        "\n",
        "    # do one gradient descent step:\n",
        "    theta -= alpha*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
        "\n",
        "    # Now: Plotting\n",
        "    # compare true function (blue) against\n",
        "    # parametrized function (orange)\n",
        "    # blue dots indicate random points where\n",
        "    # the true function was sampled in this step\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,2))\n",
        "\n",
        "    nlevels=20\n",
        "    ax[0].contourf(X,Y,C,nlevels)\n",
        "    ax[0].contour(X,Y,C,nlevels,colors=\"white\")\n",
        "    ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
        "    ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
        "    ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
        "    ax[0].set_xlabel(\"theta_0\")\n",
        "    ax[0].set_ylabel(\"theta_1\")\n",
        "\n",
        "    ax[1].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
        "    ax[1].scatter(x,true_f(x),color=\"blue\")\n",
        "    ax[1].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
        "    ax[1].set_xlim(-4,4)\n",
        "    ax[1].set_ylim(0.0,4.0)\n",
        "    ax[1].set_xlabel(\"x\")\n",
        "    ax[1].set_ylabel(\"f\")\n",
        "\n",
        "    plt.show()\n",
        "    sleep(0.3)\n",
        "\n",
        "    print(theta) #print the final fitted values"
      ],
      "metadata": {
        "id": "DE18JdYFwVY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HZZ**"
      ],
      "metadata": {
        "id": "4zJvYkwzGgxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "# plot loss vs epoch\n",
        "plt.figure(figsize=(15, 10))\n",
        "ax = plt.subplot(2, 2, 1)\n",
        "ax.plot(history.history[\"loss\"], label=\"loss\")\n",
        "ax.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "ax.legend(loc=\"upper right\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(\"loss\")\n",
        "\n",
        "# plot accuracy vs epoch\n",
        "ax = plt.subplot(2, 2, 2)\n",
        "ax.plot(history.history[\"accuracy\"], label=\"acc\")\n",
        "ax.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(\"acc\")\n",
        "\n",
        "# Plot ROC\n",
        "Y_predict = model.predict(X_test)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "ax = plt.subplot(2, 2, 3)\n",
        "ax.plot(fpr, tpr, lw=2, color=\"cyan\", label=\"auc = %.3f\" % (roc_auc))\n",
        "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"random chance\")\n",
        "ax.set_xlim([0, 1.0])\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.set_xlabel(\"false positive rate\")\n",
        "ax.set_ylabel(\"true positive rate\")\n",
        "ax.set_title(\"receiver operating curve\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ebohhgOXGimX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}